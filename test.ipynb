{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flask import Flask, request, render_template\n",
    "import joblib\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from transformers import BertTokenizer, TFBertModel ,TFAutoModel\n",
    "import xgboost\n",
    "\n",
    "xgb = joblib.load('model3/xgb_model.joblib')\n",
    "model_name = 'bert-large-cased'\n",
    "\n",
    "minMaxDiff = 33752\n",
    "minVal = 708\n",
    "\n",
    "currentIndex = 0\n",
    "\n",
    "# ['laughingTime','jokeLength','sentiment','sentiment_prob']\n",
    "df = pd.DataFrame(columns=['text','laughingTime','jokeLength','sentiment','sentiment_prob','rank'])\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "\n",
    "model2 = TFBertModel.from_pretrained(model_name)\n",
    "\n",
    "base_model = TFAutoModel.from_pretrained('bert-base-uncased')\n",
    "base_model.trainable = False\n",
    "\n",
    "input_ids = tf.keras.layers.Input(shape=(512,), dtype=tf.int32, name='input_ids')\n",
    "attention_mask = tf.keras.layers.Input(shape=(512,), dtype=tf.int32, name='attention_mask')\n",
    "\n",
    "x = base_model({\"input_ids\": input_ids, \"attention_mask\": attention_mask})[1]\n",
    "outputs = tf.keras.layers.Dense(1, activation='sigmoid')(x)\n",
    "model = tf.keras.Model(inputs={\"input_ids\": input_ids, \"attention_mask\": attention_mask}, outputs=outputs)\n",
    "model.load_weights('model/model20_minmax_extradata/my_model')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def get_rank(input):\n",
    "    global xgb\n",
    "    score = xgb.predict(input)\n",
    "    return score\n",
    "\n",
    "def getSentiment(text):\n",
    "    global tokenizer,model2\n",
    "    tokens = tokenizer.encode_plus(text, max_length=512, truncation=True, padding='max_length', return_tensors='tf')\n",
    "    output = model2(tokens)\n",
    "    pooled_output = output[1]\n",
    "    sentiment_logits = tf.keras.layers.Dense(3, activation='softmax')(pooled_output)\n",
    "    sentiment_probabilities = tf.nn.softmax(sentiment_logits, axis=1).numpy().squeeze()\n",
    "    sentiment_label = tf.argmax(sentiment_probabilities).numpy().item()\n",
    "\n",
    "    # Map the sentiment label to the corresponding sentiment\n",
    "    # sentiment_map = {0: 'Negative', 1: 'Positive', 2: 'Neutral'}\n",
    "    return [sentiment_label, sentiment_probabilities[sentiment_label]]\n",
    "\n",
    "# {'laughingTime':,'jokeLength':,'sentiment':,'sentiment_prob':}\n",
    "def addRowToDF(inp):\n",
    "    global df\n",
    "    new_row = pd.DataFrame(inp)\n",
    "\n",
    "    df = pd.concat([df, new_row], ignore_index=True)\n",
    "\n",
    "    new_row_index = df.index[-1]\n",
    "\n",
    "    return new_row_index\n",
    "\n",
    "\n",
    "\n",
    "def dataPreProcess(texts):\n",
    "    # Load the BERT tokenizer\n",
    "    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "    # Define the maximum sequence length for padding/truncating\n",
    "    max_length = 512\n",
    "\n",
    "    # Tokenize the list of strings and convert them to input IDs, attention masks, and token type IDs\n",
    "    input_ids = []\n",
    "    attention_masks = []\n",
    "\n",
    "    for text in texts:\n",
    "        # Tokenize the text and add the special [CLS] and [SEP] tokens\n",
    "        encoded_dict = tokenizer.encode_plus(\n",
    "                            text,                      # Text to encode\n",
    "                            add_special_tokens = True, # Add [CLS] and [SEP] tokens\n",
    "                            max_length = max_length,   # Pad/truncate to a maximum length\n",
    "                            pad_to_max_length = True,\n",
    "                            return_attention_mask = True,   # Generate attention masks\n",
    "                            return_token_type_ids = False,   # Do not generate token type IDs\n",
    "                            truncation=True,\n",
    "                            )\n",
    "        \n",
    "        # Add the encoded sequence and attention mask to the lists\n",
    "        input_ids.append(encoded_dict['input_ids'])\n",
    "        attention_masks.append(encoded_dict['attention_mask'])\n",
    "\n",
    "    # Convert the lists to tensors\n",
    "    input_ids = tf.convert_to_tensor(input_ids, dtype=tf.int32)\n",
    "    attention_masks = tf.convert_to_tensor(attention_masks, dtype=tf.int32)\n",
    "    \n",
    "    # Return a tuple of input IDs and attention masks\n",
    "    return input_ids, attention_masks\n",
    "\n",
    "def get_laugh_Duration(text):\n",
    "    global minMaxDiff,minVal,model\n",
    "    data = dataPreProcess([text])\n",
    "    inputs = {'input_ids': data[0], 'attention_mask': data[1]}\n",
    "    preds = model.predict(inputs).tolist()\n",
    "    val_preds = [((item[0]*minMaxDiff)+minVal) for item in list(preds)]\n",
    "    return val_preds\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \" pick up the box get back on when you hit the box together and subtract your own weight I'm going slow down hold on professor I know this guy's never tried this because I tried it and you still can't see the numbers  one of my mr. Olympia\"\n",
    "currentIndex = addRowToDF([{'text':text,'laughingTime':'','jokeLength':len(text.split()),'sentiment':'','sentiment_prob':'','rank':''}])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/deepaknandula/miniforge3/envs/env_tf/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2339: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 372ms/step\n"
     ]
    }
   ],
   "source": [
    "duration = get_laugh_Duration(df['text'][currentIndex])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "currentIndex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[5491.5219403505325]"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "duration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['laughingTime'][currentIndex] = duration[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment = getSentiment(df['text'][currentIndex])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 0.37211284]"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['sentiment'][currentIndex] = sentiment[0]\n",
    "df['sentiment_prob'][currentIndex] = sentiment[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['rank'][currentIndex] = get_rank(pd.DataFrame(df.iloc[currentIndex].to_dict(), index=[0])[['laughingTime','jokeLength','sentiment','sentiment_prob']])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>laughingTime</th>\n",
       "      <th>jokeLength</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>sentiment_prob</th>\n",
       "      <th>rank</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>pick up the box get back on when you hit the ...</td>\n",
       "      <td>5491.52194</td>\n",
       "      <td>48</td>\n",
       "      <td>2</td>\n",
       "      <td>0.372113</td>\n",
       "      <td>3.368503</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>pick up the box get back on when you hit the ...</td>\n",
       "      <td>5491.52194</td>\n",
       "      <td>48</td>\n",
       "      <td>2</td>\n",
       "      <td>0.372113</td>\n",
       "      <td>3.368503</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text laughingTime jokeLength  \\\n",
       "0   pick up the box get back on when you hit the ...   5491.52194         48   \n",
       "1   pick up the box get back on when you hit the ...   5491.52194         48   \n",
       "\n",
       "  sentiment sentiment_prob      rank  \n",
       "0         2       0.372113  3.368503  \n",
       "1         2       0.372113  3.368503  "
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.16 ('env_tf')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e7f7a6c7a328c331394eefecd3ffebe18a341a9e21c7b3790bdb1b050ca99952"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
